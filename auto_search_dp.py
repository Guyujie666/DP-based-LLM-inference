import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
import multiprocessing as mp
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import numpy as np
from tqdm import tqdm
import time
from collections import Counter, defaultdict
import math
from torch.distributions import Gamma
import argparse
import json
import os,tempfile
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch.nn as nn
PANGU_PATH = os.getenv("PANGU_PATH", "/default/pangu/path")
os.environ["TOKENIZERS_PARALLELISM"] = "false"

clip_dict = {
    'openPangu-Embedded-7B-V1.1': 0.05,

}
def get_pretrained_model(args):
    if args.base_model =="stevhliu/my_awesome_model":
        base_model = AutoModelForSequenceClassification.from_pretrained(args.base_model)
        tokenizer = AutoTokenizer.from_pretrained(args.base_model)
    elif 'pangu' in args.base_model.lower():
        model_local_path = f"{PANGU_PATH}/{args.base_model}"


        # load the tokenizer and the model
        tokenizer = AutoTokenizer.from_pretrained(
        model_local_path, 
        use_fast=False, 
        trust_remote_code=True,
        local_files_only=True
        )
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            model_local_path,
            trust_remote_code=True,
            torch_dtype="auto",
            device_map="cuda",
            local_files_only=True
        )
    return tokenizer, base_model
def get_token_embedding(token_id, model, args, squeeze=False):
    """get the token embedding given the input ids"""
    with torch.no_grad():
        if args.base_model =="stevhliu/my_awesome_model":
            embeddings = model.distilbert.embeddings.word_embeddings(token_id)
            # embeddings = model.distilbert.embeddings(token_id)
        elif 'llama' in args.base_model or 'pangu' in args.base_model.lower():
            original_device = token_id.device
            embed_layer = model.get_input_embeddings()   # é€šç”¨å†™æ³•
            device = embed_layer.weight.device   # è·å– embedding æƒé‡çš„ device
            embeddings = embed_layer(token_id.to(device))
            embeddings = embeddings.to(original_device)  # è½¬å›åŸå§‹ device
        if squeeze:
            embeddings = embeddings.squeeze()
    return embeddings
def get_closest_token(embedding, tokenizer, model, args):
    """Find the word with the closest embedding."""
    closest_token = None
    if 'gpt2' in args.base_model:
        vocabulary = tokenizer.get_vocab()
    else:
        vocabulary = tokenizer.vocab
    token_ids = [token_id for _, token_id in vocabulary.items()]
    token_ids = torch.tensor(token_ids).to(args.device)
    word_embeddings = get_token_embedding(token_ids, model, args, squeeze=True)
    # word_embeddings = torch.sign(word_embeddings)

    embedding = embedding.unsqueeze(dim=0)
    embedding = embedding.expand(word_embeddings.size())
    # distance = torch.norm(embedding - word_embeddings, 2, dim=1)
    cos_similarity = F.cosine_similarity(embedding, word_embeddings, dim=1)

    # å¦‚æœéœ€è¦è·ç¦»è€Œä¸æ˜¯ç›¸ä¼¼åº¦ï¼Œå¯ä»¥è½¬æ¢ä¸ºä½™å¼¦è·ç¦»
    distance = 1 - cos_similarity

    # closest_distances, closest_indices = torch.topk(torch.abs(word_embeddings.reshape(-1)), k=100, largest=True)
    # print(f"Closest distances: {closest_distances}")
    closest_idx = distance.argmin()
    closest_token = token_ids[closest_idx]
    # _visualize_embeddings_3d(embedding[0:1], word_embeddings, closest_indices, closest_distances, 
    #                      token_ids, vocabulary, args)
    return closest_token.item()

def sample_noise_Gaussian(d_shape, noise_stddev, device="cpu"):
    noise = torch.normal(mean=0., std=float(noise_stddev), size=d_shape, device=device)
    return noise

def _str_key(x):
    # å°†é‡åŒ–ä½æ•°/æ”»å‡»ç‡ä½œä¸ºå­—ç¬¦ä¸²é”®ï¼Œé¿å…æµ®ç‚¹ç²¾åº¦é—®é¢˜
    if isinstance(x, float):
        return f"{x:.6f}".rstrip('0').rstrip('.')
    return str(x)
def _merge_mech_table_in_place(dst_table: dict, src_table: dict, mode: str = "append"):
    """
    å°† src_table åˆå¹¶è¿› dst_tableï¼ˆä¸¤è€…ç»“æ„ç›¸åŒï¼š
      {quant_level(str): {target_attack_rate(str): record}}
    ï¼‰
    mode:
      - "append": ä»…è¡¥é½ç¼ºå¤±ç»„åˆï¼›è‹¥å·²å­˜åœ¨åˆ™â€œä¿ç•™æ—§å€¼ä¸å˜â€
      - "overwrite": è¦†ç›–å·²æœ‰ç»„åˆï¼ˆå¯ä½œä¸ºå¯é€‰å¼€å…³ï¼‰
      - "skip": å®Œå…¨è·³è¿‡å·²æœ‰ quant_level çš„ç»„åˆï¼ˆä»…æ–°å¢æ–°çš„ quant_levelï¼‰
    """
    assert mode in ("append", "overwrite", "skip")
    for qkey, ta_map in src_table.items():
        if qkey not in dst_table:
            dst_table[qkey] = {}
        if mode == "skip" and qkey in dst_table:
            # æ•´ä¸ª quant_level å·²å­˜åœ¨ï¼Œè·³è¿‡
            continue
        for tkey, rec in ta_map.items():
            if mode == "overwrite":
                dst_table[qkey][tkey] = rec
            else:  # append
                if tkey not in dst_table[qkey]:
                    dst_table[qkey][tkey] = rec
                # å·²å­˜åœ¨åˆ™ä¸æ”¹åŠ¨

def build_canonical_table(results: dict) -> dict:
    """
    å°† auto_search_privacy_parameters_comprehensive è¿”å›çš„åµŒå¥— results
    è§„æ•´ä¸º {mechanism: {quant_level(str): {target_attack_rate(str): record}}}
    record ç¤ºä¾‹:
      {
        "param_name": "mu",
        "param_value": 12.34,
        "actual_attack_rate": 0.101,
        "actual_success_rate": 0.899,
        "attack_rate_error": 0.001,
        "success_rate_error": 0.001,
        "dp_rounds": 7,
        "quant_level": 4,
        "target_attack_rate": 0.1,
        "target_success_rate": 0.9,
        "search_time": 12.3
      }
    """
    table = {}
    for ta, qdict in results.items():
        for ql, mechdict in qdict.items():
            for mech, rec in mechdict.items():
                mech_tbl = table.setdefault(mech, {})
                qkey = _str_key(ql)
                ta_key = _str_key(ta)
                qtbl = mech_tbl.setdefault(qkey, {})
                if "error" in rec:
                    qtbl[ta_key] = {"error": rec["error"]}
                else:
                    row = {
                        "param_name": rec["parameter_name"],
                        "param_value": rec["best_parameter"],
                        "actual_attack_rate": rec["actual_attack_rate"],
                        "actual_success_rate": rec["actual_success_rate"],
                        "attack_rate_error": rec["attack_rate_difference"],
                        "success_rate_error": rec["success_rate_difference"],
                        "dp_rounds": rec.get("dp_rounds", None),
                        "quant_level": ql,
                        "target_attack_rate": ta,
                        "target_success_rate": rec["target_success_rate"],
                        "search_time": rec.get("search_time", 0.0),
                    }
                    qtbl[ta_key] = row
    return table

def write_results_per_mechanism(
    canonical_table: dict,
    *,
    output_dir: str,
    dataset: str,
    model_name_or_path: str,
    proj_dim: int,
    emb_ckpt: str,
    extra_meta: dict = None,
    merge_mode: str = "append",   # "append" | "overwrite" | "skip"
):
    """
    å°†è§„èŒƒåŒ–ç»“æœæŒ‰æœºåˆ¶æ‹†åˆ†å†™å…¥æ–‡ä»¶ï¼Œå¸¦â€œå¢é‡åˆå¹¶â€ï¼š
    - è‹¥æ–‡ä»¶ä¸å­˜åœ¨ï¼šç›´æ¥å†™å…¥
    - è‹¥æ–‡ä»¶å­˜åœ¨ï¼šè¯»å‡ºåæŒ‰ merge_mode åˆå¹¶ï¼Œä»…â€œappendâ€æ—¶ä¸ä¼šæ”¹æ—§å€¼
    """

    out_root = Path(output_dir)
    out_root.mkdir(parents=True, exist_ok=True)
    safe_model = safe_tag(model_name_or_path)

    for mechanism, new_mech_table in canonical_table.items():
        filename = f"{dataset}__{safe_model}__proj{proj_dim}__{mechanism}.json"
        out_path = out_root / filename

        # 1) å‡†å¤‡ payloadï¼ˆé»˜è®¤åªå« new çš„ tableï¼‰
        payload = {
            "dataset": dataset,
            "model": model_name_or_path,
            "proj_dim": proj_dim,
            "mechanism": mechanism,
            "emb_ckpt": emb_ckpt,
            "meta": {
                "time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "merge_mode": merge_mode,
            },
            "table": new_mech_table,
        }
        if extra_meta:
            payload["meta"].update(extra_meta)

        # 2) è‹¥å·²æœ‰æ–‡ä»¶ï¼Œè¯»å–å¹¶åˆå¹¶
        if out_path.exists():
            try:
                with open(out_path, "r") as f:
                    old = json.load(f)
                # å…ƒä¿¡æ¯ï¼šä¿ç•™æœ€æ—©ä¸€æ¬¡çš„ dataset/model/proj_dim ç­‰ï¼Œæ›´æ–° meta.time
                payload["dataset"] = old.get("dataset", payload["dataset"])
                payload["model"] = old.get("model", payload["model"])
                payload["proj_dim"] = old.get("proj_dim", payload["proj_dim"])
                payload["emb_ckpt"] = old.get("emb_ckpt", payload["emb_ckpt"])
                old_table = old.get("table", {})

                # åˆå¹¶è¡¨
                _merge_mech_table_in_place(old_table, new_mech_table, mode=merge_mode)
                payload["table"] = old_table
            except Exception as e:
                print(f"[WARN] Failed to read/merge existing file {out_path}: {e}. Writing fresh file.")

        # 3) åŸå­å†™ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†æ›¿æ¢
        tmp_fd, tmp_path = tempfile.mkstemp(prefix=f".tmp_{filename}.", dir=str(out_root))
        try:
            with os.fdopen(tmp_fd, "w") as tmpf:
                json.dump(payload, tmpf, indent=2, ensure_ascii=False)
                tmpf.flush()
                os.fsync(tmpf.fileno())
            os.replace(tmp_path, out_path)  # POSIX åŸå­æ›¿æ¢
        finally:
            # è‹¥å¼‚å¸¸ï¼Œæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_path):
                try: os.remove(tmp_path)
                except: pass

        print(f"[OK] wrote (merge={merge_mode}) â†’ {out_path}")

def safe_tag(s: str) -> str:
    s = s.strip().replace("\\", "/")
    for ch in ["/", ":", " ", "@", "#", "?", "&", "=", "+"]:
        s = s.replace(ch, "__")
    return s
def write_search_json(
    output_dir: str,
    dataset: str,
    model_name_or_path: str,
    proj_dim: int,
    mechanism: str,
    quant_level: int,
    emb_ckpt: str,
    results: list,
    extra_meta: dict = None
):
    """
    results: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ {attack_rate, param_value, achieved, iters, status}
    """
    out_root = Path(output_dir)
    out_root.mkdir(parents=True, exist_ok=True)

    safe_model = safe_tag(model_name_or_path)
    filename = f"{dataset}__{safe_model}__proj{proj_dim}__{mechanism}_q{quant_level}.json"
    out_path = out_root / filename

    payload = {
        "dataset": dataset,
        "model": model_name_or_path,
        "proj_dim": proj_dim,
        "mechanism": mechanism,
        "quant_level": quant_level,
        "emb_ckpt": emb_ckpt,
        "targets": results,
        "meta": {
            "time": time.strftime("%Y-%m-%d %H:%M:%S"),
        },
    }
    if extra_meta:
        payload["meta"].update(extra_meta)

    with open(out_path, "w") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)

    print(f"[OK] wrote search result â†’ {out_path}")

# [Keep all your existing helper functions: sign_flip_noise, quantize_tensor, etc.]
@torch.no_grad()
def build_vocab_matrix(tokenizer, model, args, dtype=torch.float16):
    # å–è¯è¡¨æ‰€æœ‰ idï¼ˆæ³¨æ„ä¸åŒtokenizerçš„æ¥å£ï¼‰
    if 'gpt2' in args.base_model.lower() or 'pangu' in args.base_model.lower():
        vocab = tokenizer.get_vocab()
        token_ids = torch.tensor([tid for _, tid in vocab.items()], device='cpu', dtype=torch.long)
    else:
        vocab = tokenizer.vocab
        token_ids = torch.tensor([tid for _, tid in vocab.items()], device='cpu', dtype=torch.long)

    # å– embeddingï¼ˆåˆ†æ‰¹æ¬åˆ° GPUï¼Œé¿å…å³°å€¼ï¼‰
    bs = 8192
    embs = []
    for i in range(0, len(token_ids), bs):
        chunk = token_ids[i:i+bs].to(args.device)
        e = get_token_embedding(chunk, model, args, squeeze=True)  # (bs, H)
        e = F.normalize(e, p=2, dim=1)                              # å…ˆå•ä½åŒ–
        embs.append(e.to(dtype))
        torch.cuda.empty_cache()
    vocab_mat = torch.cat(embs, dim=0)                              # (V, H) on GPU
    return token_ids.to(args.device), vocab_mat                      # å½’ä¸€åŒ–ä¸”å¸¸é©»GPU


def init_proj_layers(model, hidden_size, proj_dim, emb_ckpt_path=None, device="cuda", dtype=torch.float32):
    """
    åœ¨ model ä¸ŠæŒ‚ä¸¤ä¸ªçº¿æ€§å±‚ï¼š
      - model.cus_proj:  hidden_size -> proj_dim
      - model.cus_deproj: proj_dim    -> hidden_size
    å¦‚æä¾› emb_ckpt_pathï¼Œåˆ™ä» ckpt åŠ è½½ state_dictï¼ˆè¦æ±‚åŒ…å« 'proj' å’Œ 'deproj'ï¼‰
    """
    model.cus_proj = nn.Linear(hidden_size, proj_dim, bias=False).to(device=device, dtype=dtype)
    model.cus_deproj = nn.Linear(proj_dim,  hidden_size, bias=False).to(device=device, dtype=dtype)

    if emb_ckpt_path:
        ckpt = torch.load(emb_ckpt_path, map_location="cpu")
        # å…¼å®¹ï¼šå¯èƒ½ç›´æ¥æ˜¯ä¸¤ä¸ª state_dictï¼›ä¹Ÿå¯èƒ½åœ¨ 'proj' / 'deproj' é”®ä¸‹
        if isinstance(ckpt, dict) and "proj" in ckpt and "deproj" in ckpt:
            proj_sd = ckpt["proj"]
            deproj_sd = ckpt["deproj"]
        else:
            # ç®€å•å…œåº•ï¼šè‹¥é¡¶å±‚å°±æ˜¯ state_dict
            proj_sd = ckpt.get("proj", ckpt)
            deproj_sd = ckpt.get("deproj", ckpt)
        model.cus_proj.load_state_dict(proj_sd, strict=True)
        model.cus_deproj.load_state_dict(deproj_sd, strict=True)

    # æ¨ç†æ¨¡å¼ï¼Œé¿å…æ¢¯åº¦ä¸ dropout ç­‰
    model.cus_proj.eval()
    model.cus_deproj.eval()
    return model


@torch.no_grad()
def project_add_noise_deproject(embeds, model, args):
    """
    embeds: [B, T, H] æˆ– [N, H]
    åœ¨æŠ•å½±ç©ºé—´åŠ å™ªï¼Œå†åæŠ•å½±å›åŸç©ºé—´ã€‚åªæ”¹å˜æ•°å€¼ï¼Œä¸æ”¹å˜å½¢çŠ¶ã€‚
    """
    assert hasattr(model, "cus_proj") and hasattr(model, "cus_deproj"), \
        "Projection layers not found. Call init_proj_layers first."

    # ç»Ÿä¸€åˆ° 3Dï¼Œä¾¿äº batch å¤„ç†
    squeeze_back = False
    if embeds.dim() == 2:
        embeds = embeds.unsqueeze(0)   # -> [1, N, H]
        squeeze_back = True

    target_device = embeds.device
    if model.cus_proj.weight.device != target_device:
        model.cus_proj = model.cus_proj.to(target_device)
        model.cus_deproj = model.cus_deproj.to(target_device)

    # æŠ•å½±
    projected = model.cus_proj(embeds)            # [B, T, D]
    # åœ¨æŠ•å½±ç©ºé—´åŠ å™ª
    if args.noise_type == 'ternary':
        noisy_proj = ternary_noise_encode(projected, args)+ projected
    elif args.noise_type == 'gaussian':
        noisy_proj = sample_noise_Gauss(projected, args) + projected
    elif args.noise_type == 'binary':
        noisy_proj = gauss_binary_noise_encode(projected, args) + projected
    elif args.noise_type in ['chidp', 'chi']:
        noisy_proj = sample_noise_Chi(projected, args) + projected
    else:
        raise ValueError(f"Unknown noise type for projection-space noise: {args.noise_type}")

    # ï¼ˆå¯é€‰ï¼‰è‹¥ä½ åªæƒ³å¯¹æŠ•å½±åçš„â€œå¢é‡â€åšé‡åŒ–ï¼Œå†åæŠ•å½±ï¼š
    if args.noise_type in ['gaussian', 'chidp'] and getattr(args, "quant_level", 32) != 32:
        noisy_proj = quantize_tensor(noisy_proj, args.quant_level)

    # åæŠ•å½±
    deproj = model.cus_deproj(noisy_proj)         # [B, T, H]

    if squeeze_back:
        deproj = deproj.squeeze(0)                # è¿˜åŸåˆ° [N, H]
    return deproj


def sign_flip_noise(vector, flip_ratio, args):
    noise_mask = torch.bernoulli(
            torch.full(vector.shape, flip_ratio, device=args.device)
        ).bool()
    return vector * (1 - 2 * noise_mask)  # ç¿»è½¬é€‰ä¸­çš„ä½
def quantize_tensor(tensor, num_bits):
    """
    éšæœºé‡åŒ–å‡½æ•° - å®ç°æ— åçš„éšæœºé‡åŒ– (Stochastic Quantization)
    
    Args:
        tensor: è¾“å…¥å¼ é‡
        num_bits: é‡åŒ–ä½æ•°
    
    Returns:
        quantized_tensor: éšæœºé‡åŒ–åçš„å¼ é‡
    """
    # æ‰¾åˆ°è¾“å…¥å¼ é‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
    # min_val, max_val = tensor.min(dim=-1, keepdim=True)[0], tensor.max(dim=-1, keepdim=True)[0]
    min_val, max_val = torch.min(tensor), torch.max(tensor)

    # è®¡ç®—é‡åŒ–çº§åˆ«çš„æ•°é‡
    q_levels = 2 ** num_bits

    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = (max_val - min_val) / (q_levels - 1)
    
    # é¿å…é™¤é›¶é”™è¯¯
    scale = torch.where(scale == 0, torch.ones_like(scale), scale)

    # å°†è¿ç»­å€¼æ˜ å°„åˆ° [0, q_levels-1] èŒƒå›´
    normalized = (tensor - min_val) / scale
    
    # éšæœºé‡åŒ–ï¼šåŸºäºå°æ•°éƒ¨åˆ†è¿›è¡Œæ¦‚ç‡æ€§èˆå…¥
    floor_vals = torch.floor(normalized)
    frac_vals = normalized - floor_vals
    
    # ç”Ÿæˆéšæœºæ•°ï¼Œå¦‚æœéšæœºæ•°å°äºå°æ•°éƒ¨åˆ†ï¼Œåˆ™å‘ä¸Šèˆå…¥ï¼Œå¦åˆ™å‘ä¸‹èˆå…¥
    random_vals = torch.rand_like(frac_vals)
    quantized = torch.where(random_vals < frac_vals, 
                           floor_vals + 1, 
                           floor_vals)
    
    # ç¡®ä¿é‡åŒ–å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
    quantized = quantized.clamp(0, q_levels - 1)
    
    # è¿˜åŸä¸ºåŸå§‹èŒƒå›´çš„æµ®ç‚¹æ•°
    quantized_tensor = quantized * scale + min_val

    return quantized_tensor

# def quantize_tensor(tensor, num_bits):
#     # æ‰¾åˆ°è¾“å…¥å¼ é‡çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
#     min_val, max_val = tensor.min(dim=1, keepdim=True)[0], tensor.max(dim=1, keepdim=True)[0]

#     # è®¡ç®—é‡åŒ–çº§åˆ«çš„æ•°é‡
#     q_levels = 2 ** num_bits

#     # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
#     scale = (max_val - min_val) / (q_levels - 1)

#     # é‡åŒ–åˆ°æ•´æ•°ï¼Œç„¶åè¿˜åŸä¸ºæµ®ç‚¹æ•°
#     quantized = ((tensor - min_val) / scale).round().clamp(0, q_levels - 1)
#     quantized_tensor = quantized * scale + min_val

#     return quantized_tensor
def sample_noise_Chi(init_emb, args):
    size= init_emb.shape
    device = args.device
    eta = args.eta
    alpha = torch.ones(*size) * size[-1]
    beta = torch.ones(*size) * eta
    m = Gamma(alpha, beta)
    l_lst = m.sample()
    # v_lst = -2 * torch.rand(*size) + 1
    v_lst = torch.randn(size)
    v_lst = v_lst / torch.norm(v_lst, dim=-1, keepdim=True)
    noise = l_lst * v_lst
    noise = noise.to(device)
    return noise

def sample_noise_Gauss(init_emb, args):

    noise_std = 2*args.clip_c_bound/args.mu*math.sqrt(init_emb.shape[-1]) #################è®°å¾—æ”¹å›å»
    # print(f"noise_std: {noise_std}")
    # print('shape of init_emb:', init_emb.shape)
    noises = sample_noise_Gaussian(init_emb.shape, noise_std, args.device)
    ####sparsity
    # random_variable = torch.rand_like(init_emb)
    # noises = torch.where(random_variable <= 1 - args.sparsity, -init_emb, noises)
    # # noise_std = args.train_noise_std if mode == "train" else args.test_noise_std
    # noise_std = 2 * 0.267/mu
    # # print("noise_std:", noise_std)
    # # print("noise_std:", noise_std)
    # noises = sample_noise_Gaussian(init_emb.shape, noise_std, device)

    return noises
def ternary_noise_encode(init_emb, args):
    encoder_list = []
    for i in range(args.dp_rounds):
        mu_ = math.sqrt(args.mu ** 2 / args.dp_rounds)/math.sqrt(init_emb.shape[-1])
        A = math.sqrt(args.sparsity * (4 / mu_ ** 2 + 1) * args.clip_c_bound ** 2)  # æ­¤å¤„çœ‹ä¸€ä¸‹init_embæœ‰æ²¡æœ‰batchç»´åº¦
        B = A / args.sparsity
        random_variable = torch.rand_like(init_emb)
        ones_tensor = B * torch.ones_like(init_emb)
        zeros_tensor = torch.zeros_like(init_emb)
        encoded_tensor = torch.where(random_variable <= (1 / 2 + init_emb / (2 * A)), ones_tensor, -ones_tensor)
        random_variable = torch.rand_like(encoded_tensor)
        encoded_tensor = torch.where(random_variable <= 1 - A / B, zeros_tensor, encoded_tensor)
        encoder_list.append(encoded_tensor)
        # A = torch.sqrt(args.sparsity * (4/args.mu ** 2 + 1)*torch.max(init_emb)**2)#æ­¤å¤„çœ‹ä¸€ä¸‹init_embæœ‰æ²¡æœ‰batchç»´åº¦
        # B = A/args.sparsity
        # random_variable = torch.rand_like(init_emb)
        # ones_tensor = B*torch.ones_like(init_emb)
        # zeros_tensor = torch.zeros_like(init_emb)
        # encoded_tensor = torch.where(random_variable <= (1 / 2 + init_emb / (2 * A)), ones_tensor, -ones_tensor)
        # random_variable = torch.rand_like(encoded_tensor)
        # encoded_tensor = torch.where(random_variable <= 1 - A / B, zeros_tensor, encoded_tensor)
    stacked_tensors = torch.stack(encoder_list)
    encoded_tensor = torch.mean(stacked_tensors, dim=0)
    noises = encoded_tensor - init_emb
    return noises

def gauss_binary_noise_encode(init_emb, args):
    encoder_list = []
    for i in range(args.dp_rounds):
        mu_ = math.sqrt(args.mu ** 2 / args.dp_rounds)
        noise_std = 2*args.clip_bound/mu_
        noises = sample_noise_Gaussian(init_emb.shape, noise_std, args.device)
        encoded = init_emb + noises
        sign_noises = torch.sign(encoded)
        encoder_list.append(sign_noises)
    stacked_tensors = torch.stack(encoder_list)
    encoded_tensor = torch.mean(stacked_tensors, dim=0)
    noises = encoded_tensor - init_emb
    return noises
class GenericTokenDataset(Dataset):
    """
    å°†æ–‡æœ¬æ•°æ®é›†ï¼ˆwikitext2 / ptbï¼‰è½¬æˆâ€œå¹²å‡€çš„ token åºåˆ—â€ï¼Œç”¨äºæœ€è¿‘é‚»è¿˜åŸè¯„ä¼°ã€‚
    - è¿‡æ»¤ç©ºè¡Œã€ä»…å–æœ‰æ•ˆåˆ†è¯ï¼ˆ<vocab_size ä¸” >3ï¼‰
    - å¯éšæœºä¸‹é‡‡æ · subset_size ä¸ª token
    """
    def __init__(self, tokenizer, dataset_name: str, subset_size: int = None, seed: int = 42):
        self.tokenizer = tokenizer
        self.dataset_name = dataset_name.lower()
        self.seed = seed

        texts = self._load_raw_texts()
        all_tokens = []
        for txt in tqdm(texts, desc=f"Processing {self.dataset_name}"):
            if not txt or not str(txt).strip():
                continue
            toks = tokenizer.encode(str(txt), add_special_tokens=False)
            valid = [t for t in toks if (t < tokenizer.vocab_size and t > 3)]
            all_tokens.extend(valid)

        # éšæœºä¸‹é‡‡æ ·
        if subset_size and len(all_tokens) > subset_size:
            rng = np.random.default_rng(seed)
            idx = rng.choice(len(all_tokens), subset_size, replace=False)
            all_tokens = [all_tokens[i] for i in idx]

        self.tokens = torch.tensor(all_tokens, dtype=torch.long)

    def _load_raw_texts(self):
        if self.dataset_name in ["wikitext2", "wikitext-2", "wikitext"]:
            ds = load_dataset("wikitext", "wikitext-2-v1", split="test")
            return ds["text"]

        if self.dataset_name in ["ptb", "penn-treebank", "ptb_text_only"]:
            # HF ä¸Šå¸¸ç”¨ï¼šptb_text_only é…ç½®çš„å­—æ®µåæ˜¯ 'sentence'
            # æœ‰äº›é•œåƒå¯èƒ½ç›´æ¥ç”¨é»˜è®¤ configï¼›åšä¸ªå…œåº•
            try:
                ds = load_dataset("ptb_text_only", "penn_treebank", split="test")
                col = "sentence"
            except Exception:
                ds = load_dataset("ptb_text_only", split="test")
                # å…œåº•æ‰¾ç¬¬ä¸€ä¸ª string åˆ—
                col = next((k for k, v in ds.features.items() if getattr(v, "dtype", None) == "string"), "sentence")
            return ds[col]

        raise ValueError(f"Unsupported dataset: {self.dataset_name}")

    def __len__(self):
        return len(self.tokens)

    def __getitem__(self, idx):
        return self.tokens[idx]


def make_eval_dataloader(tokenizer, args):
    """
    æ ¹æ® args.dataset æ„é€  DataLoader
    """
    ds = GenericTokenDataset(
        tokenizer=tokenizer,
        dataset_name=args.dataset,
        subset_size=args.subset_size or args.test_size  # ä¸ test_size å¯¹é½
    )
    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=False, num_workers=2)
    return ds, loader


@torch.no_grad()
def batch_get_closest_token(embeddings, tokenizer, model, args, batch_size=256,
                            token_ids_all=None, vocab_mat=None, vocab_chunk=16384):
    device = args.device
    N, H = embeddings.shape
    # å½’ä¸€åŒ–å¹¶å¯¹é½ dtype
    X = F.normalize(embeddings.to(device), p=2, dim=1).to(vocab_mat.dtype)

    # ç”¨ -inf æˆ–è¯¥ dtype çš„æœ€å°æœ‰é™å€¼åˆå§‹åŒ–
    winners_idx = torch.empty(N, dtype=torch.long, device=device)
    winners_val = torch.full((N,), float('-inf'), dtype=vocab_mat.dtype, device=device)
    # æˆ–è€…ï¼š
    # winners_val = torch.full((N,), torch.finfo(vocab_mat.dtype).min,
    #                          dtype=vocab_mat.dtype, device=device)

    V = vocab_mat.size(0)
    for s in range(0, V, vocab_chunk):
        e = min(s + vocab_chunk, V)
        W = vocab_mat[s:e]  # (chunk, H)

        for i in range(0, N, batch_size):
            j = min(i + batch_size, N)
            # åœ¨ fp32 åš matmulï¼Œç»“æœå† cast å› vocab_mat.dtypeï¼ˆæ¨èï¼‰
            sims = (X[i:j].to(torch.float32) @ W.to(torch.float32).t()).to(vocab_mat.dtype)
            best_val, best_col = sims.max(dim=1)
            update = best_val > winners_val[i:j]
            winners_val[i:j] = torch.where(update, best_val, winners_val[i:j])
            winners_idx[i:j] = torch.where(update, best_col + s, winners_idx[i:j])

        del W
        torch.cuda.empty_cache()

    return token_ids_all[winners_idx].tolist()



def parallel_noise_generation(embeddings, args, num_workers=None):
    """å¹¶è¡Œç”Ÿæˆå™ªå£°"""
    if num_workers is None:
        num_workers = min(mp.cpu_count(), 8)
    
    def generate_noise_chunk(embedding_chunk):
        if args.noise_type == 'ternary':
            return ternary_noise_encode(embedding_chunk, args)
        elif args.noise_type == 'gaussian':
            return sample_noise_Gauss(embedding_chunk, args)
        elif args.noise_type == 'binary':
            return gauss_binary_noise_encode(embedding_chunk, args)
        elif args.noise_type == 'chi':
            return sample_noise_Chi(embedding_chunk, args)
        else:
            raise ValueError(f"Unknown noise type: {args.noise_type}")
    
    # åˆ†å—å¤„ç†
    chunk_size = len(embeddings) // num_workers
    chunks = [embeddings[i:i+chunk_size] for i in range(0, len(embeddings), chunk_size)]
    
    # å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        noise_chunks = list(executor.map(generate_noise_chunk, chunks))
    
    return torch.cat(noise_chunks, dim=0)

def evaluate_defense_success_rate(tokenizer, model, test_loader, args, test_size=1000, token_ids_all=None, vocab_mat=None):
    """
    è¯„ä¼°ç»™å®šå‚æ•°ä¸‹çš„é˜²å¾¡æˆåŠŸç‡
    
    Args:
        tokenizer: é¢„è®­ç»ƒçš„tokenizer
        model: é¢„è®­ç»ƒçš„æ¨¡å‹
        test_loader: æ•°æ®åŠ è½½å™¨
        args: å‚æ•°é…ç½®
        test_size: æµ‹è¯•æ ·æœ¬æ•°é‡
    
    Returns:
        defense_success_rate: é˜²å¾¡æˆåŠŸç‡
    """
    print(f"Evaluating with {args.noise_type}, quant_level={args.quant_level}, "
          f"{'mu=' + str(args.mu) if args.noise_type in ['gaussian', 'ternary'] else 'eta=' + str(args.eta)}")
    
    defense_successes = 0
    total_samples = 0
    samples_processed = 0
    
    with torch.no_grad():
        for batch_tokens in test_loader:
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¶³å¤Ÿçš„æ ·æœ¬
            if samples_processed >= test_size:
                break
                
            batch_tokens = batch_tokens.to(args.device)
            
            # é™åˆ¶batchå¤§å°ä»¥ä¸è¶…è¿‡test_size
            remaining_samples = test_size - samples_processed
            if len(batch_tokens) > remaining_samples:
                batch_tokens = batch_tokens[:remaining_samples]
            
            # è·å–åŸå§‹åµŒå…¥
            original_embeddings = get_token_embedding(batch_tokens, model, args)
            
            # åº”ç”¨è£å‰ª
            if args.noise_type in ['binary']:
                all_norms = torch.norm(original_embeddings, p=2, dim=-1, keepdim=True)
                scaling_factor = torch.clamp(args.clip_bound / all_norms, max=1.0)
                clipped_embeddings = original_embeddings * scaling_factor
            elif args.noise_type in ['gaussian', 'ternary']:
                clipped_embeddings = torch.clamp(
                    original_embeddings, 
                    min=-args.clip_c_bound, 
                    max=args.clip_c_bound
                )
            else:  # chi
                clipped_embeddings = original_embeddings
            
            # ç”Ÿæˆå™ªå£°
            if args.noise_type == 'ternary':
                noises = ternary_noise_encode(clipped_embeddings, args)
            elif args.noise_type == 'gaussian':
                noises = sample_noise_Gauss(clipped_embeddings, args)
            elif args.noise_type == 'binary':
                noises = gauss_binary_noise_encode(clipped_embeddings, args)
            elif args.noise_type == 'chidp':
                noises = sample_noise_Chi(clipped_embeddings, args)
            
            # ==== æŠ•å½±ç©ºé—´åŠ å™ªï¼šè‹¥å¼€å¯äº†æŠ•å½± ====
            if hasattr(model, "cus_proj") and hasattr(model, "cus_deproj"):
                # æ³¨æ„ï¼šè¿™é‡Œå»ºè®®å¯¹â€œè£å‰ªåçš„å‘é‡â€è¿›è¡ŒæŠ•å½±â†’åŠ å™ªâ†’åæŠ•å½±
                noisy_embeddings = project_add_noise_deproject(clipped_embeddings, model, args)
            else:
                # ==== åŸæ¥çš„ç›´æ¥åœ¨åŸç©ºé—´åŠ å™ª ====
                noisy_embeddings = clipped_embeddings + noises
                if args.noise_type in ['gaussian', 'chidp'] and args.quant_level != 32:
                    noisy_embeddings = quantize_tensor(noisy_embeddings, args.quant_level)

            
            predicted_tokens = batch_get_closest_token(
                noisy_embeddings, tokenizer, model, args,
                batch_size=256,
                token_ids_all=token_ids_all,
                vocab_mat=vocab_mat,
                vocab_chunk=16384
            )

            
            original_tokens = batch_tokens.cpu().tolist()
            for orig_token, pred_token in zip(original_tokens, predicted_tokens):
                total_samples += 1
                if orig_token != pred_token:
                    defense_successes += 1
            
            samples_processed += len(batch_tokens)
    
    defense_success_rate = defense_successes / total_samples
    return defense_success_rate

def binary_search_privacy_parameter(tokenizer, model, test_loader, args, target_success_rate, 
                                   tolerance=0.02, max_iterations=15, test_size=2000, token_ids_all=None, vocab_mat=None):
    """
    ä½¿ç”¨äºŒåˆ†æœç´¢æ‰¾åˆ°è¾¾åˆ°ç›®æ ‡é˜²å¾¡æˆåŠŸç‡çš„éšç§å‚æ•°
    
    Args:
        tokenizer: é¢„è®­ç»ƒçš„tokenizer
        model: é¢„è®­ç»ƒçš„æ¨¡å‹
        test_loader: æ•°æ®åŠ è½½å™¨
        args: å‚æ•°é…ç½®
        target_success_rate: ç›®æ ‡é˜²å¾¡æˆåŠŸç‡ (0-1)
        tolerance: å®¹å¿è¯¯å·®
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        test_size: æµ‹è¯•æ ·æœ¬æ•°é‡
    
    Returns:
        best_param: æœ€ä¼˜å‚æ•°å€¼
        best_success_rate: å®é™…è¾¾åˆ°çš„æˆåŠŸç‡
    """
    print(f"\n=== å¼€å§‹æœç´¢ {args.noise_type} æœºåˆ¶çš„éšç§å‚æ•° ===")
    print(f"ç›®æ ‡é˜²å¾¡æˆåŠŸç‡: {target_success_rate:.3f}")
    print(f"é‡åŒ–çº§åˆ«: {args.quant_level}")
    
    # æ ¹æ®ä¸åŒæœºåˆ¶è®¾ç½®æœç´¢èŒƒå›´
    if args.noise_type == 'gaussian':
        param_name = 'mu'
        low, high = 1.0, 100.0  # muçš„æœç´¢èŒƒå›´
    elif args.noise_type == 'ternary':
        param_name = 'mu'
        low, high = 1.0, 2000.0  # muçš„æœç´¢èŒƒå›´
        # if 'qwen' in args.base_model.lower():
        #     if args.quant_level < 2:
        #         high = 1000.0  # Qwenæ¨¡å‹éœ€è¦æ›´å¤§çš„muèŒƒå›´
        #     else:
        #         high = 400.0  # Qwenæ¨¡å‹éœ€è¦æ›´å¤§çš„muèŒƒå›´
    elif args.noise_type == 'chidp':
        param_name = 'eta'
        low, high = 10.0, 1000.0  # etaçš„æœç´¢èŒƒå›´
    else:
        raise ValueError(f"Unsupported noise type: {args.noise_type}")
    
    best_param = None
    best_success_rate = None
    best_diff = float('inf')
    
    print(f"æœç´¢èŒƒå›´: {param_name} âˆˆ [{low}, {high}]")
    print("-" * 60)
    
    for iteration in range(max_iterations):
        mid = (low + high) / 2.0
        
        # è®¾ç½®å‚æ•°
        if param_name == 'mu':
            args.mu = mid
        else:  # eta
            args.eta = mid
        
        # å¦‚æœæ˜¯ternaryï¼Œè¿˜éœ€è¦è®¾ç½®dp_rounds
        if args.noise_type == 'ternary':
            args.dp_rounds = 2**args.quant_level - 1
        
        # è¯„ä¼°å½“å‰å‚æ•°ä¸‹çš„é˜²å¾¡æˆåŠŸç‡ï¼ˆä¼ å…¥å·²åˆå§‹åŒ–çš„æ¨¡å‹å’Œæ•°æ®ï¼‰
        current_success_rate = evaluate_defense_success_rate(
            tokenizer, model, test_loader, args, test_size, token_ids_all, vocab_mat
        )
        
        diff = abs(current_success_rate - target_success_rate)
        
        print(f"è¿­ä»£ {iteration+1:2d}: {param_name}={mid:6.2f} -> "
              f"æˆåŠŸç‡={current_success_rate:.4f} (ç›®æ ‡={target_success_rate:.4f}, "
              f"å·®è·={diff:.4f})")
        
        # æ›´æ–°æœ€ä½³ç»“æœ
        if diff < best_diff:
            best_diff = diff
            best_param = mid
            best_success_rate = current_success_rate
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å®¹å¿è¯¯å·®
        if diff <= tolerance:
            print(f"âœ“ åœ¨ç¬¬ {iteration+1} æ¬¡è¿­ä»£è¾¾åˆ°ç›®æ ‡ï¼")
            break
        
        # è°ƒæ•´æœç´¢èŒƒå›´
        if current_success_rate < target_success_rate:
            # æˆåŠŸç‡å¤ªä½ï¼Œéœ€è¦å‡å°å‚æ•°ï¼ˆå‡å°‘å™ªå£°ï¼‰
            high = mid
        else:
            # æˆåŠŸç‡å¤ªé«˜ï¼Œéœ€è¦å¢å¤§å‚æ•°ï¼ˆå¢åŠ å™ªå£°ï¼‰
            low = mid
    
    print("-" * 60)
    print(f"æœç´¢å®Œæˆï¼")
    print(f"æœ€ä½³å‚æ•°: {param_name} = {best_param:.4f}")
    print(f"å®é™…æˆåŠŸç‡: {best_success_rate:.4f}")
    print(f"ä¸ç›®æ ‡å·®è·: {abs(best_success_rate - target_success_rate):.4f}")
    
    return best_param, best_success_rate

def auto_search_privacy_parameters_comprehensive(args, target_attack_rates, quant_levels, privacy_mechanisms, 
                                               tolerance=0.02, max_iterations=15, test_size=2000, batch_size=32):
    """
    å…¨é¢çš„è‡ªåŠ¨æœç´¢å¤šç§é…ç½®ä¸‹çš„éšç§å‚æ•°
    
    Args:
        args: åŸºç¡€å‚æ•°é…ç½®
        target_attack_rates: ç›®æ ‡æ”»å‡»ç‡åˆ—è¡¨ (ä¾‹å¦‚ [0.1, 0.2, 0.3])
        quant_levels: é‡åŒ–çº§åˆ«åˆ—è¡¨ (ä¾‹å¦‚ [4, 8, 16])
        privacy_mechanisms: éšç§æœºåˆ¶åˆ—è¡¨ (ä¾‹å¦‚ ['gaussian', 'ternary', 'chidp'])
        tolerance: å®¹å¿è¯¯å·®
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        test_size: æµ‹è¯•æ ·æœ¬æ•°é‡
        batch_size: æ‰¹å¤„ç†å¤§å°
    
    Returns:
        results: æœç´¢ç»“æœå­—å…¸
    """
    
    # ç¡®ä¿è¾“å…¥å‚æ•°éƒ½æ˜¯åˆ—è¡¨å½¢å¼
    if not isinstance(target_attack_rates, list):
        target_attack_rates = [target_attack_rates]
    if not isinstance(quant_levels, list):
        quant_levels = [quant_levels]
    if not isinstance(privacy_mechanisms, list):
        privacy_mechanisms = [privacy_mechanisms]
    
    print(f"\n{'='*100}")
    print(f"ğŸš€ å…¨é¢è‡ªåŠ¨æœç´¢éšç§å‚æ•°")
    print(f"{'='*100}")
    print(f"ç›®æ ‡æ”»å‡»ç‡: {target_attack_rates}")
    print(f"é‡åŒ–çº§åˆ«: {quant_levels}")
    print(f"éšç§æœºåˆ¶: {privacy_mechanisms}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {test_size}, æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # ====== åˆå§‹åŒ–æ¨¡å‹ & è¯è¡¨çŸ©é˜µ ======
    print("\nğŸ“¦ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œæ•°æ®é›†...")
    tokenizer, model = get_pretrained_model(args)
    token_ids_all, vocab_mat = build_vocab_matrix(tokenizer, model, args, dtype=torch.float16)

    # model = model.to(args.device)
    hidden_size = model.get_input_embeddings().weight.size(1)

    if args.proj_dim is not None and args.emb_ckpt is not None:
        init_proj_layers(
            model,
            hidden_size=hidden_size,
            proj_dim=args.proj_dim,
            emb_ckpt_path=args.emb_ckpt,
            device=args.device,
            dtype=torch.float32 if model.dtype is torch.float32 else model.dtype
        )
        print(f"[proj] enabled: H={hidden_size} -> D={args.proj_dim}, ckpt={args.emb_ckpt}")
    else:
        print("[proj] disabled (proj_dim or emb_ckpt not provided)")

    model.eval()

    # ====== ä½¿ç”¨é€šç”¨æ•°æ®é›†å°è£…ï¼ˆæ”¯æŒ wikitext2 / ptbï¼‰======
    _, test_loader = make_eval_dataloader(tokenizer, args)
    print(f"âœ… æ•°æ®é›† {args.dataset} åˆå§‹åŒ–å®Œæˆï¼ï¼ˆsubset_size={args.subset_size or args.test_size}ï¼‰")

    
    results = {}
    total_configs = len(target_attack_rates) * len(quant_levels) * len(privacy_mechanisms)
    current_config = 0
    
    for target_attack_rate in target_attack_rates:
        target_success_rate = 1.0 - target_attack_rate
        
        print(f"\n{'='*80}")
        print(f"ğŸ¯ ç›®æ ‡æ”»å‡»ç‡: {target_attack_rate:.3f} (é˜²å¾¡æˆåŠŸç‡: {target_success_rate:.3f})")
        print(f"{'='*80}")
        
        results[target_attack_rate] = {}
        
        for quant_level in quant_levels:
            print(f"\nğŸ“Š é‡åŒ–çº§åˆ«: {quant_level}")
            results[target_attack_rate][quant_level] = {}
            args.quant_level = quant_level
            
            for mechanism in privacy_mechanisms:
                current_config += 1
                print(f"\nğŸ”’ [{current_config}/{total_configs}] éšç§æœºåˆ¶: {mechanism}")
                args.noise_type = mechanism
                

                start_time = time.time()
                
                # ä½¿ç”¨å·²åˆå§‹åŒ–çš„æ¨¡å‹å’Œæ•°æ®é›†è¿›è¡Œæœç´¢
                best_param, actual_success_rate = binary_search_privacy_parameter(
                    tokenizer, model, test_loader, args, target_success_rate, 
                    tolerance, max_iterations, test_size, token_ids_all, vocab_mat
                )
                
                search_time = time.time() - start_time
                actual_attack_rate = 1.0 - actual_success_rate
                
                results[target_attack_rate][quant_level][mechanism] = {
                    'target_attack_rate': target_attack_rate,
                    'target_success_rate': target_success_rate,
                    'actual_attack_rate': actual_attack_rate,
                    'actual_success_rate': actual_success_rate,
                    'best_parameter': best_param,
                    'parameter_name': 'mu' if mechanism in ['gaussian', 'ternary'] else 'eta',
                    'attack_rate_difference': abs(actual_attack_rate - target_attack_rate),
                    'success_rate_difference': abs(actual_success_rate - target_success_rate),
                    'search_time': search_time
                }
                
                print(f"âœ… æœç´¢æˆåŠŸï¼ç”¨æ—¶: {search_time:.2f}ç§’")
                    

    
    return results

def print_comprehensive_search_summary(results):
    """æ‰“å°å…¨é¢æœç´¢ç»“æœæ‘˜è¦"""
    print(f"\n{'='*100}")
    print("ğŸ¯ å…¨é¢æœç´¢ç»“æœæ‘˜è¦")
    print(f"{'='*100}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_configs = 0
    successful_configs = 0
    failed_configs = 0
    
    for target_attack_rate, quant_data in results.items():
        print(f"\nğŸ¯ ç›®æ ‡æ”»å‡»ç‡: {target_attack_rate:.3f} (é˜²å¾¡æˆåŠŸç‡: {1.0-target_attack_rate:.3f})")
        print("=" * 80)
        
        for quant_level, mechanisms in quant_data.items():
            print(f"\nğŸ“Š é‡åŒ–çº§åˆ« {quant_level}:")
            print("-" * 60)
            
            for mechanism, result in mechanisms.items():
                total_configs += 1
                
                if 'error' in result:
                    failed_configs += 1
                    print(f"  âŒ {mechanism:10s}: æœç´¢å¤±è´¥ - {result['error']}")
                else:
                    successful_configs += 1
                    param_name = result['parameter_name']
                    param_value = result['best_parameter']
                    actual_attack_rate = result['actual_attack_rate']
                    target_attack_rate = result['target_attack_rate']
                    attack_diff = result['attack_rate_difference']
                    search_time = result.get('search_time', 0)
                    
                    print(f"  âœ… {mechanism:10s}: {param_name}={param_value:6.2f} -> "
                          f"æ”»å‡»ç‡={actual_attack_rate:.4f} (ç›®æ ‡={target_attack_rate:.4f}, "
                          f"è¯¯å·®={attack_diff:.4f}, ç”¨æ—¶={search_time:.1f}s)")
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»é…ç½®æ•°: {total_configs}")
    print(f"  æˆåŠŸé…ç½®: {successful_configs} ({successful_configs/total_configs*100:.1f}%)")
    print(f"  å¤±è´¥é…ç½®: {failed_configs} ({failed_configs/total_configs*100:.1f}%)")
    print(f"{'='*80}")

def export_results_to_table(results, filename="privacy_search_results.csv"):
    """å°†æœç´¢ç»“æœå¯¼å‡ºä¸ºCSVè¡¨æ ¼"""
    import pandas as pd
    
    data = []
    for target_attack_rate, quant_data in results.items():
        for quant_level, mechanisms in quant_data.items():
            for mechanism, result in mechanisms.items():
                if 'error' not in result:
                    data.append({
                        'Target_Attack_Rate': target_attack_rate,
                        'Target_Success_Rate': result['target_success_rate'],
                        'Quantization_Level': quant_level,
                        'Privacy_Mechanism': mechanism,
                        'Parameter_Name': result['parameter_name'],
                        'Best_Parameter': result['best_parameter'],
                        'Actual_Attack_Rate': result['actual_attack_rate'],
                        'Actual_Success_Rate': result['actual_success_rate'],
                        'Attack_Rate_Error': result['attack_rate_difference'],
                        'Success_Rate_Error': result['success_rate_difference'],
                        'Search_Time': result.get('search_time', 0)
                    })
    
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"ğŸ“Š ç»“æœå·²å¯¼å‡ºåˆ°: {filename}")
    return df


def export_results_index(
    canonical_table: dict,
    *,
    output_dir: str,
    dataset: str,
    model_name_or_path: str,
    proj_dim: int,
):
    safe_model = safe_tag(model_name_or_path)
    out_path = Path(output_dir) / f"{dataset}__{safe_model}__proj{proj_dim}__index.json"
    rows = []
    for mech, qtbl in canonical_table.items():
        for qkey, tdict in qtbl.items():
            for tkey, rec in tdict.items():
                r = {"mechanism": mech, "quant_level": qkey, "target_attack_rate": tkey}
                r.update(rec)
                rows.append(r)
    tmp_fd, tmp_path = tempfile.mkstemp(prefix=".tmp_index.", dir=str(Path(output_dir)))
    try:
        with os.fdopen(tmp_fd, "w") as tmpf:
            json.dump({"rows": rows}, tmpf, indent=2, ensure_ascii=False)
            tmpf.flush(); os.fsync(tmpf.fileno())
        os.replace(tmp_path, out_path)
    finally:
        if os.path.exists(tmp_path):
            try: os.remove(tmp_path)
            except: pass
    print(f"[OK] wrote index â†’ {out_path}")

def str2type(v):
    """Util function for user friendly boolean flag args"""
    if isinstance(v, torch.dtype):
        return v
    if "float32" in v.lower():
        return torch.float32
    elif "float16" in v.lower():
        return torch.float16

def parse_args():
    parser = argparse.ArgumentParser(description="Comprehensive privacy parameter auto-search")

    # åŸºç¡€æ¨¡å‹ä¸è®¾å¤‡
    parser.add_argument("--base_model", type=str, default="baffo32/decapoda-research-llama-7b-hf")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--proj_dim", type=int, default=128)
    parser.add_argument("--emb_ckpt", type=str, default=None)

    # æ•°æ®é›†è®¾ç½®
    parser.add_argument("--dataset", type=str, default="wikitext2",
                        choices=["wikitext2", "ptb"],
                        help="é€‰æ‹©è¯„ä¼°æ•°æ®é›†ï¼šwikitext2 æˆ– ptb (Penn Treebank text-only)")
    parser.add_argument("--subset_size", type=int, default=2000,
                        help="ä»æµ‹è¯•é›†éšæœºæŠ½æ ·çš„ token æ•°é‡ä¸Šé™ï¼Œç”¨äºæœç´¢è¯„ä¼°")

    # éšç§æœºåˆ¶ä¸é‡åŒ–
    parser.add_argument("--noise_type", type=str, default="ternary",
                        choices=["gaussian", "ternary", "chidp", "binary"])
    parser.add_argument("--mu", type=float, default=10.0)
    parser.add_argument("--eta", type=float, default=100.0)
    parser.add_argument("--dp_rounds", type=int, default=1)
    parser.add_argument("--quant_level", type=int, default=4)

    # è£å‰ª/ç¨€ç–
    parser.add_argument("--sparsity", type=float, default=1.0)
    parser.add_argument("--clip_bound", type=float, default=1.0)
    parser.add_argument("--clip_c_bound", type=float, default=None)

    # æœç´¢è®¾ç½®
    parser.add_argument("--target_attack_rates", type=float, nargs="+",
                        default=[0.02, 0.1, 0.2, 0.4, 0.6])
    parser.add_argument("--quant_levels", type=int, nargs="+", default=[4])
    parser.add_argument("--privacy_mechanisms", type=str, nargs="+", default=["ternary"])
    parser.add_argument("--tolerance", type=float, default=0.01)
    parser.add_argument("--max_iterations", type=int, default=20)
    parser.add_argument("--test_size", type=int, default=2000)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--base_precision", type=str2type, default=torch.float32,
                    help = "Precision of base model")

    # è¾“å‡º
    parser.add_argument("--output_dir", type=str, default="/data/dp_soft_prompt/search_results")

    return parser.parse_args()


def main():
    args = parse_args()

    # è‡ªåŠ¨å¡«å…… clip_c_boundï¼ˆå¦‚ä½ ä¹‹å‰æ‰€åšï¼‰
    if getattr(args, "clip_c_bound", None) is None and args.base_model in clip_dict:
        args.clip_c_bound = clip_dict[args.base_model]
    # if args.proj_dim==128 and 'llama' in args.base_model.lower() and args.emb_ckpt is None:
    #     args.clip_c_bound = 0.1
    # ä½ å·²æœ‰çš„æœç´¢è¿‡ç¨‹
    print("å¼€å§‹å…¨é¢è‡ªåŠ¨éšç§å‚æ•°æœç´¢...")
    results = auto_search_privacy_parameters_comprehensive(
        args=args,
        target_attack_rates=args.target_attack_rates,
        quant_levels=args.quant_levels,
        privacy_mechanisms=args.privacy_mechanisms,
        tolerance=args.tolerance,
        max_iterations=args.max_iterations,
        test_size=args.test_size,
        batch_size=args.batch_size
    )

    # æ‘˜è¦
    print_comprehensive_search_summary(results)

    # è§„èŒƒåŒ–ç»“æœä¸º {mechanism -> quant_level(str) -> target_attack_rate(str) -> row}
    canonical_table = build_canonical_table(results)

    # canonical_table = build_canonical_table(results)
    write_results_per_mechanism(
        canonical_table,
        output_dir=args.output_dir,
        dataset=args.dataset,
        model_name_or_path=args.base_model,
        proj_dim=args.proj_dim,
        emb_ckpt=args.emb_ckpt,
        extra_meta={"quant_levels": args.quant_levels, "targets": args.target_attack_rates},
        merge_mode="overwrite",   # â† è¿™é‡Œæ§åˆ¶åˆå¹¶ç­–ç•¥
    )
    export_results_index(
        canonical_table,
        output_dir=args.output_dir,
        dataset=args.dataset,
        model_name_or_path=args.base_model,
        proj_dim=args.proj_dim
    )

if __name__ == "__main__":
    # è¿è¡Œä¸»æœç´¢ç¨‹åº
    main()
    
